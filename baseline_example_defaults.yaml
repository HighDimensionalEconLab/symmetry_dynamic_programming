trainer:
  max_epochs: 1000
  min_epochs: 0
  max_time: 00:00:15:00
  precision: 32
  num_sanity_val_steps: 0
  callbacks:
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
        log_momentum: true
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        filename: best
        monitor: val_loss
        verbose: false
        save_last: true
        save_top_k: 1
        save_weights_only: true
        mode: min
        auto_insert_metric_name: true
    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: val_loss
        min_delta: 0.0
        patience: 50
        mode: min
        check_finite: true
        divergence_threshold: 100000 # stops if larger
        stopping_threshold: 1.0e-6
optimizer:
  class_path: torch.optim.Adam
  init_args:
    lr: 5.0e-3
# Temporarily setting this scheduler, but it isn't tuned
lr_scheduler:
  class_path: torch.optim.lr_scheduler.StepLR
  init_args:
    step_size: 100
    gamma: 0.1
# lr_scheduler:
#   class_path: torch.optim.lr_scheduler.ReduceLROnPlateau
#   init_args:
#     factor: 0.1
#     mode: min
#     patience: 5
model:
  # Model parameters
  N: 128
  alpha_0: 1.0
  alpha_1: 1.0
  beta: 0.95
  gamma: 90.0
  sigma: 0.005
  delta: 0.05
  eta: 0.001
  nu: 1.0
  zeta: 1.0

  # Settings for method
  verbose: true
  W_quadrature_nodes: 7
  normalize_shock_vector: true
  train_trajectories: 16
  val_trajectories: 8
  test_trajectories: 32
  batch_size: 32 # set to 0 for full dataset
  T: 63
  X_0_loc: 0.9
  X_0_scale: 0.05

  # Settings for neural networks
  phi_layers: 2
  phi_dim: 128
  phi_bias: true
  rho_layers: 4
  rho_dim: 128
  rho_bias: true
  L: 4
